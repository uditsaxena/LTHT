{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('..')\n",
    "\n",
    "import networkx as nx\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import scipy.sparse\n",
    "import matplotlib.pyplot as plt\n",
    "from archs.mnist.LeNet5 import LeNet5\n",
    "\n",
    "import persim # see persim.scikit-tda.org\n",
    "from ripser import ripser # see ripser.scikit-tda.org"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate the model used in this example, I ran:\n",
    "\n",
    "`python main.py --arch_type lenet5 --dataset mnist --prune_percent 90 --prune_iterations 2 --end_iter 20`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global-like variable definitions.\n",
    "model_name = 'lenet5'\n",
    "dataset_name = 'mnist'\n",
    "data_location = '../data' # location of training data (MNIST, FashionMNIST, CIFAR, etc.)\n",
    "msk_pct = 90 # prune percentage for LT network\n",
    "model_loc0 = 'saves/{}/{}/0_model_lt.pth.tar'.format(model_name, dataset_name) # location of saved, un-pruned model \n",
    "model_loc1 = 'saves/{}/{}/1_model_lt.pth.tar'.format(model_name, dataset_name) # location of saved, pruned model (after 1 prune iteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unpruned Model Homology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model.\n",
    "model = torch.load(model_loc0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter Shape:  (64, 1, 3, 3)\n",
      "Parameter Shape:  (64, 64, 3, 3)\n",
      "Parameter Shape:  (256, 12544)\n",
      "Parameter Shape:  (256, 256)\n",
      "Parameter Shape:  (10, 256)\n"
     ]
    }
   ],
   "source": [
    "ln5 = LeNet5()\n",
    "\n",
    "# access model params and add to list `params`. We are ignoring bias weights for now.\n",
    "params = []\n",
    "for name, param in model.named_parameters():\n",
    "    if 'weight' in name:\n",
    "        pnum = param.data.cpu().numpy()\n",
    "        print('Parameter Shape: ', pnum.shape)\n",
    "        params.append(pnum)\n",
    "\n",
    "# add weights to conv and linear layer param info dict\n",
    "param_info = ln5.param_info\n",
    "for i in range(len(param_info)):\n",
    "    p = param_info[i]\n",
    "    if p['layer_type'] == 'Conv2d' or p['layer_type'] == 'Linear':\n",
    "        p['param'] = params[i]\n",
    "    else:\n",
    "        p['param'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expected size of input layer\n",
    "input_size = (1,1,28,28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_im2col_indices(x_shape, field_height, field_width, padding=1, stride=1):\n",
    "    # First figure out what the size of the output should be\n",
    "    N, C, H, W = x_shape\n",
    "    assert (H + 2 * padding - field_height) % stride == 0\n",
    "    assert (W + 2 * padding - field_height) % stride == 0\n",
    "    out_height = int((H + 2 * padding - field_height) / stride + 1)\n",
    "    out_width = int((W + 2 * padding - field_width) / stride + 1)\n",
    "\n",
    "    i0 = np.repeat(np.arange(field_height), field_width)\n",
    "    i0 = np.tile(i0, C)\n",
    "    i1 = stride * np.repeat(np.arange(out_height), out_width)\n",
    "    j0 = np.tile(np.arange(field_width), field_height * C)\n",
    "    j1 = stride * np.tile(np.arange(out_width), out_height)\n",
    "    i = i0.reshape(-1, 1) + i1.reshape(1, -1)\n",
    "    j = j0.reshape(-1, 1) + j1.reshape(1, -1)\n",
    "\n",
    "    k = np.repeat(np.arange(C), field_height * field_width).reshape(-1, 1)\n",
    "\n",
    "    return (k.astype(int), i.astype(int), j.astype(int))\n",
    "\n",
    "def im2col_indices(x, field_height, field_width, padding=1, stride=1):\n",
    "    \"\"\" An implementation of im2col based on some fancy indexing \"\"\"\n",
    "    # Zero-pad the input\n",
    "    p = padding\n",
    "    x_padded = np.pad(x, ((0, 0), (0, 0), (p, p), (p, p)), mode='constant')\n",
    "\n",
    "    k, i, j = get_im2col_indices(x.shape, field_height, field_width, padding, stride)\n",
    "\n",
    "    cols = x_padded[:, k, i, j]\n",
    "    C = x.shape[1]\n",
    "    cols = cols.transpose(1, 2, 0).reshape(field_height * field_width * C, -1)\n",
    "    return cols\n",
    "\n",
    "def col2im_indices(cols, x_shape, field_height=3, field_width=3, padding=1,\n",
    "                   stride=1):\n",
    "    \"\"\" An implementation of col2im based on fancy indexing and np.add.at \"\"\"\n",
    "    N, C, H, W = x_shape\n",
    "    H_padded, W_padded = H + 2 * padding, W + 2 * padding\n",
    "    x_padded = np.zeros((N, C, H_padded, W_padded), dtype=cols.dtype)\n",
    "    k, i, j = get_im2col_indices(x_shape, field_height, field_width, padding, stride)\n",
    "    cols_reshaped = cols.reshape(C * field_height * field_width, -1, N)\n",
    "    cols_reshaped = cols_reshaped.transpose(2, 0, 1)\n",
    "    np.add.at(x_padded, (slice(None), k, i, j), cols_reshaped)\n",
    "    if padding == 0:\n",
    "        return x_padded\n",
    "    return x_padded[:, :, padding:-padding, padding:-padding]\n",
    "\n",
    "def conv_layer_as_matrix(X, X_names, W, stride, padding):\n",
    "    n_filters, d_filter, h_filter, w_filter = W.shape\n",
    "    n_x, d_x, h_x, w_x = X.shape\n",
    "    h_out = (h_x - h_filter + 2 * padding) / stride + 1\n",
    "    w_out = (w_x - w_filter + 2 * padding) / stride + 1\n",
    "    \n",
    "    X_col = im2col_indices(X, h_filter, w_filter, padding=padding, stride=stride)\n",
    "    X_names_col = im2col_indices(X_names, h_filter, w_filter, padding=padding, stride=stride)\n",
    "    W_col = W.reshape(n_filters, -1)\n",
    "    \n",
    "    out = W_col @ X_col\n",
    "    out = out.reshape(n_filters, int(h_out), int(w_out), n_x)\n",
    "    out = out.transpose(3, 0, 1, 2)\n",
    "    \n",
    "    return out, X_col, W_col, X_names_col\n",
    "\n",
    "def inverse_abs(x):\n",
    "    return np.abs(1/x)\n",
    "\n",
    "def add_conv(G, input_size, p, name_this, name_next, stride, padding, weight_func=inverse_abs, next_linear=False):\n",
    "    '''adds convolutional layer to graph and returns updated graph'''\n",
    "    conv_format = '{}_{}_{}'\n",
    "    input_channels = p.shape[1]\n",
    "    X = np.ones(input_size)\n",
    "    for c in range(input_channels):\n",
    "        print('Channel: {}'.format(c))\n",
    "        X_names = np.arange(X.shape[2]*X.shape[3]).reshape((1,1,X.shape[2],X.shape[3]))\n",
    "        tx = X[:,c,:,:].reshape((X.shape[0],1,X.shape[2],X.shape[3]))\n",
    "        # convert to matrix information\n",
    "        mat, X_col, W_col, xnames = conv_layer_as_matrix(tx,X_names,p[:,c,:,:].reshape((p.shape[0],1,p.shape[2],p.shape[3])),stride,padding)\n",
    "        for f in range(W_col.shape[0]):\n",
    "            for row in range(X_col.shape[0]):\n",
    "                for col in range(X_col.shape[1]):\n",
    "                    v = W_col[f,row]\n",
    "                    if v != 0:\n",
    "                        if next_linear:\n",
    "                            # next layer is linear\n",
    "                            G.add_edge(conv_format.format(name_this,c,xnames[row,col]),conv_format.format(name_next,0,int((X_col.shape[1]*c) + (f*X_col.shape[1]) + col)), weight=weight_func(v))\n",
    "                        else:\n",
    "                            # next layer is conv\n",
    "                            G.add_edge(conv_format.format(name_this,c,xnames[row,col]),conv_format.format(name_next,f,col), weight=weight_func(v))\n",
    "    input_size = mat.shape\n",
    "    return G, input_size\n",
    "\n",
    "def add_mp(G, input_size, name_this, name_next, kernel_size, stride, padding, next_linear=False):\n",
    "    '''adds max pooling layer to graph and returns updated graph'''\n",
    "    conv_format = '{}_{}_{}'\n",
    "    p = np.ones((input_size[0],input_size[1],kernel_size[0],kernel_size[1]))\n",
    "    input_channels = p.shape[1]\n",
    "    # next layer also conv\n",
    "    X = np.ones(input_size)\n",
    "    for c in range(input_channels):\n",
    "        print('Channel: {}'.format(c))\n",
    "        X_names = np.arange(X.shape[2]*X.shape[3]).reshape((1,1,X.shape[2],X.shape[3]))\n",
    "        tx = X[:,c,:,:].reshape((X.shape[0],1,X.shape[2],X.shape[3]))\n",
    "        # convert to matrix information\n",
    "        mat, X_col, W_col, xnames = conv_layer_as_matrix(tx,X_names,p[:,c,:,:].reshape((p.shape[0],1,p.shape[2],p.shape[3])),stride,padding)\n",
    "        for f in range(W_col.shape[0]):\n",
    "            for row in range(X_col.shape[0]):\n",
    "                for col in range(X_col.shape[1]):\n",
    "                    node_name = conv_format.format(name_this,c,xnames[row,col])\n",
    "                    ews = sorted(G.in_edges(node_name, data=True), key=lambda x: x[2]['weight'])\n",
    "                    if len(ews) > 0:\n",
    "                        v = ews[0][2]['weight']\n",
    "                        if v != 0:\n",
    "                            if next_linear:\n",
    "                                G.add_edge(conv_format.format(name_this,c,xnames[row,col]),conv_format.format(name_next,0,int((X_col.shape[1]*c) + (f*X_col.shape[1]) + col)), weight=v)\n",
    "                            else:\n",
    "                                G.add_edge(conv_format.format(name_this,c,xnames[row,col]),conv_format.format(name_next,f,col), weight=v)\n",
    "    input_size = mat.shape\n",
    "    return G, input_size\n",
    "\n",
    "def add_linear_linear(G, p, name_this, name_next, weight_func=inverse_abs):\n",
    "    '''adds linear layer to graph and returns updated graph'''\n",
    "    conv_format = '{}_{}_{}'\n",
    "    for row in range(p.shape[1]):\n",
    "        for col in range(p.shape[0]):\n",
    "            v = p[col,row]\n",
    "            if v != 0:\n",
    "                G.add_edge(conv_format.format(name_this,0,row),conv_format.format(name_next,0,col), weight=weight_func(v))\n",
    "    return G\n",
    "\n",
    "def to_directed_networkx(params, input_size):\n",
    "    # store all network info here\n",
    "    G = nx.DiGraph()\n",
    "    \n",
    "    # bookkeeping matrix for pooling layers\n",
    "    mat = None\n",
    "    \n",
    "    # assume last layer linear \n",
    "    for l in range(len(params)-1):\n",
    "        \n",
    "        param = params[l]\n",
    "        # need to look ahead at next layer to get naming correct\n",
    "        param_next = params[l+1]\n",
    "        \n",
    "        print('Layer: {}'.format(param['name']))\n",
    "        \n",
    "        if param['layer_type'] == 'Conv2d':\n",
    "            \n",
    "            if param_next['layer_type'] == 'Conv2d' or param_next['layer_type'] == 'MaxPool2d':\n",
    "                \n",
    "                G, input_size = add_conv(G, input_size, param['param'], param['name'], param_next['name'], param['stride'], param['padding'], next_linear=False)\n",
    "                \n",
    "            elif param_next['layer_type'] == 'Linear':\n",
    "                \n",
    "                G, input_size = add_conv(G, input_size, param['param'], param['name'], param_next['name'], param['stride'], param['padding'], next_linear=True)\n",
    "                \n",
    "        elif param['layer_type'] == 'MaxPool2d':\n",
    "            \n",
    "            if param_next['layer_type'] == 'Conv2d':\n",
    "                \n",
    "                G, input_size = add_mp(G, input_size, param['name'], param_next['name'], param['kernel_size'], param['stride'], param['padding'], next_linear=False)\n",
    "                \n",
    "            if param_next['layer_type'] == 'Linear':\n",
    "                \n",
    "                G, input_size = add_mp(G, input_size, param['name'], param_next['name'], param['kernel_size'], param['stride'], param['padding'], next_linear=True)\n",
    "                \n",
    "        elif param['layer_type'] == 'Linear':\n",
    "            # linear layer\n",
    "            G = add_linear_linear(G, param['param'], param['name'], param_next['name'])\n",
    "            \n",
    "        else:\n",
    "            raise ValueError('Layer type not implemented ')\n",
    "            \n",
    "    # add in last layer\n",
    "    print('Layer: {}'.format(params[-1]['name']))\n",
    "    G = add_linear_linear(G, params[-1]['param'], params[-1]['name'], 'Output')\n",
    "    \n",
    "    return G\n",
    "\n",
    "\n",
    "# helper function for testing model, outputs accuracy\n",
    "def test(model, test_loader, criterion):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.data.max(1, keepdim=True)[1]  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.data.view_as(pred)).sum().item()\n",
    "        test_loss /= len(test_loader.dataset)\n",
    "        accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: Conv1\n",
      "Channel: 0\n",
      "Layer: Conv2\n",
      "Channel: 0\n",
      "Channel: 1\n",
      "Channel: 2\n",
      "Channel: 3\n",
      "Channel: 4\n",
      "Channel: 5\n",
      "Channel: 6\n",
      "Channel: 7\n",
      "Channel: 8\n",
      "Channel: 9\n",
      "Channel: 10\n",
      "Channel: 11\n",
      "Channel: 12\n",
      "Channel: 13\n",
      "Channel: 14\n",
      "Channel: 15\n",
      "Channel: 16\n",
      "Channel: 17\n",
      "Channel: 18\n",
      "Channel: 19\n",
      "Channel: 20\n",
      "Channel: 21\n",
      "Channel: 22\n",
      "Channel: 23\n",
      "Channel: 24\n",
      "Channel: 25\n",
      "Channel: 26\n",
      "Channel: 27\n",
      "Channel: 28\n",
      "Channel: 29\n",
      "Channel: 30\n",
      "Channel: 31\n",
      "Channel: 32\n",
      "Channel: 33\n",
      "Channel: 34\n",
      "Channel: 35\n",
      "Channel: 36\n",
      "Channel: 37\n",
      "Channel: 38\n",
      "Channel: 39\n",
      "Channel: 40\n",
      "Channel: 41\n",
      "Channel: 42\n",
      "Channel: 43\n",
      "Channel: 44\n",
      "Channel: 45\n",
      "Channel: 46\n",
      "Channel: 47\n",
      "Channel: 48\n",
      "Channel: 49\n",
      "Channel: 50\n",
      "Channel: 51\n",
      "Channel: 52\n",
      "Channel: 53\n",
      "Channel: 54\n",
      "Channel: 55\n",
      "Channel: 56\n",
      "Channel: 57\n",
      "Channel: 58\n",
      "Channel: 59\n",
      "Channel: 60\n",
      "Channel: 61\n",
      "Channel: 62\n",
      "Channel: 63\n",
      "Layer: MaxPool1\n",
      "Channel: 0\n",
      "Channel: 1\n",
      "Channel: 2\n",
      "Channel: 3\n",
      "Channel: 4\n",
      "Channel: 5\n",
      "Channel: 6\n",
      "Channel: 7\n",
      "Channel: 8\n",
      "Channel: 9\n",
      "Channel: 10\n",
      "Channel: 11\n",
      "Channel: 12\n",
      "Channel: 13\n",
      "Channel: 14\n",
      "Channel: 15\n",
      "Channel: 16\n",
      "Channel: 17\n",
      "Channel: 18\n",
      "Channel: 19\n",
      "Channel: 20\n",
      "Channel: 21\n",
      "Channel: 22\n",
      "Channel: 23\n",
      "Channel: 24\n",
      "Channel: 25\n",
      "Channel: 26\n",
      "Channel: 27\n",
      "Channel: 28\n",
      "Channel: 29\n",
      "Channel: 30\n",
      "Channel: 31\n",
      "Channel: 32\n",
      "Channel: 33\n",
      "Channel: 34\n",
      "Channel: 35\n",
      "Channel: 36\n",
      "Channel: 37\n",
      "Channel: 38\n",
      "Channel: 39\n",
      "Channel: 40\n",
      "Channel: 41\n",
      "Channel: 42\n",
      "Channel: 43\n",
      "Channel: 44\n",
      "Channel: 45\n",
      "Channel: 46\n",
      "Channel: 47\n",
      "Channel: 48\n",
      "Channel: 49\n",
      "Channel: 50\n",
      "Channel: 51\n",
      "Channel: 52\n",
      "Channel: 53\n",
      "Channel: 54\n",
      "Channel: 55\n",
      "Channel: 56\n",
      "Channel: 57\n",
      "Channel: 58\n",
      "Channel: 59\n",
      "Channel: 60\n",
      "Channel: 61\n",
      "Channel: 62\n",
      "Channel: 63\n",
      "Layer: Linear1\n",
      "Layer: Linear2\n"
     ]
    }
   ],
   "source": [
    "# this will take a while for a network with a lot of channels...\n",
    "# look into parallelizing this\n",
    "G = to_directed_networkx(param_info, input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 97.76\n"
     ]
    }
   ],
   "source": [
    "# Test to make sure model works.\n",
    "transform=transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.1307,), (0.3081,))])\n",
    "testdataset = datasets.MNIST(data_location, train=False, transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(testdataset, batch_size=32, shuffle=False, num_workers=0,drop_last=True)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "print('Accuracy: {}'.format(test(model, test_loader, criterion)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Conv1_0_479',\n",
       " 'Conv2_28_507',\n",
       " 'MaxPool1_0_508',\n",
       " 'Linear1_0_128',\n",
       " 'Linear2_0_175',\n",
       " 'Output_0_8']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# out of curiosity...\n",
    "nx.dag_longest_path(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dgms': [array([[0.        , 0.76375628],\n",
       "         [0.        , 0.76375628],\n",
       "         [0.        , 0.76375628],\n",
       "         ...,\n",
       "         [0.        ,        inf],\n",
       "         [0.        ,        inf],\n",
       "         [0.        ,        inf]]),\n",
       "  array([[       inf,        inf],\n",
       "         [       inf,        inf],\n",
       "         [       inf,        inf],\n",
       "         ...,\n",
       "         [0.91383654,        inf],\n",
       "         [0.91383654,        inf],\n",
       "         [0.91383654,        inf]])],\n",
       " 'cocycles': [[], []],\n",
       " 'num_edges': 28520919,\n",
       " 'dperm2all': <113946x113946 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 28526911 stored elements in Compressed Sparse Row format>,\n",
       " 'idx_perm': array([     0,      1,      2, ..., 113943, 113944, 113945]),\n",
       " 'r_cover': 0.0}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute rips persistent homology (up to 1st dimension) over entire network \n",
    "# using (sparse) adjacency matrix as distance matrix.\n",
    "rips = ripser(nx.to_scipy_sparse_matrix(G), distance_matrix=True, maxdim=1)\n",
    "rips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot persistence diagram in dimensions 0 and 1 (on same axes).\n",
    "# points at infinity (homology groups) are plotted on the dotted \n",
    "# line which represents the point \\infty.\n",
    "persim.plot_diagrams(rips['dgms'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruned LT Homology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the LT model.\n",
    "model_lt = torch.load(model_loc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter Shape:  (64, 1, 3, 3)\n",
      "Parameter Shape:  (64, 64, 3, 3)\n",
      "Parameter Shape:  (256, 12544)\n",
      "Parameter Shape:  (256, 256)\n",
      "Parameter Shape:  (10, 256)\n"
     ]
    }
   ],
   "source": [
    "ln5 = LeNet5()\n",
    "\n",
    "# access model params and add to list `params`. We are ignoring bias weights for now.\n",
    "params_lt = []\n",
    "for name, param in model_lt.named_parameters():\n",
    "    if 'weight' in name:\n",
    "        pnum = param.data.cpu().numpy()\n",
    "        print('Parameter Shape: ', pnum.shape)\n",
    "        params_lt.append(pnum)\n",
    "        \n",
    "param_info_lt = ln5.param_info\n",
    "for i in range(len(param_info_lt)):\n",
    "    p = param_info_lt[i]\n",
    "    if p['layer_type'] == 'Conv2d' or p['layer_type'] == 'Linear':\n",
    "        p['param'] = params_lt[i]\n",
    "    else:\n",
    "        p['param'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 97.28\n"
     ]
    }
   ],
   "source": [
    "# test this model's accuracy.\n",
    "print('Accuracy: {}'.format(test(model_lt, test_loader, criterion)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: Conv1\n",
      "Channel: 0\n",
      "Layer: Conv2\n",
      "Channel: 0\n",
      "Channel: 1\n",
      "Channel: 2\n",
      "Channel: 3\n",
      "Channel: 4\n",
      "Channel: 5\n",
      "Channel: 6\n",
      "Channel: 7\n",
      "Channel: 8\n",
      "Channel: 9\n",
      "Channel: 10\n",
      "Channel: 11\n",
      "Channel: 12\n",
      "Channel: 13\n",
      "Channel: 14\n",
      "Channel: 15\n",
      "Channel: 16\n",
      "Channel: 17\n",
      "Channel: 18\n",
      "Channel: 19\n",
      "Channel: 20\n",
      "Channel: 21\n",
      "Channel: 22\n",
      "Channel: 23\n",
      "Channel: 24\n",
      "Channel: 25\n",
      "Channel: 26\n",
      "Channel: 27\n",
      "Channel: 28\n",
      "Channel: 29\n",
      "Channel: 30\n",
      "Channel: 31\n",
      "Channel: 32\n",
      "Channel: 33\n",
      "Channel: 34\n",
      "Channel: 35\n",
      "Channel: 36\n"
     ]
    }
   ],
   "source": [
    "# compute networkx representation of LT NN.\n",
    "G_lt = to_directed_networkx(param_info_lt, input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute rips persistent homology (up to 1st dimension) over entire network \n",
    "# using (sparse) adjacency matrix as distance matrix.\n",
    "rips_lt = ripser(nx.to_scipy_sparse_matrix(G_lt), distance_matrix=True, maxdim=1)\n",
    "rips_lt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot persistence diagram in dimensions 0 and 1 (on same axes).\n",
    "# points at infinity (homology groups) are plotted on the dotted \n",
    "# line which represents the point \\infty.\n",
    "persim.plot_diagrams(rips_lt['dgms'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bottleneck distance is pretty restrictive with large PDs like this..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
