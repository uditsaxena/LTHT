{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import scipy.sparse\n",
    "import matplotlib.pyplot as plt\n",
    "from nn_homology import nn_graph\n",
    "\n",
    "import persim # see persim.scikit-tda.org\n",
    "from ripser import ripser # see ripser.scikit-tda.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global-like variable definitions.\n",
    "model_name = 'lenet5_nmp'\n",
    "dataset_name = 'cifar10'\n",
    "data_location = '../data' # location of training data (MNIST, FashionMNIST, CIFAR, etc.)\n",
    "seed = 42 # prune percentage for LT network\n",
    "model_loc0 = 'remote_saves/{}/{}/{}/0/model_lt_20.pth.tar'.format(model_name, dataset_name, seed) # location of saved, un-pruned model \n",
    "model_loc1 = 'remote_saves/{}/{}/{}/9/model_lt_20.pth.tar'.format(model_name, dataset_name, seed) # location of saved, pruned model (after 1 prune iteration)\n",
    "input_size = (1,3,32,32)\n",
    "from archs.cifar10.LeNet5 import LeNet5_nmp as Mc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unpruned Model Homology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model.\n",
    "model = torch.load(model_loc0)\n",
    "model_class = Mc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NNG = nn_graph.NNGraph()\n",
    "NNG.parameter_graph(model, model_class.param_info, input_size, ignore_zeros=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function for testing model, outputs accuracy\n",
    "def test(model, test_loader, criterion):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.data.max(1, keepdim=True)[1]  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.data.view_as(pred)).sum().item()\n",
    "        test_loss /= len(test_loader.dataset)\n",
    "        accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test to make sure model works.\n",
    "transform=transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.1307,), (0.3081,))])\n",
    "testdataset = datasets.CIFAR10(data_location, train=False, transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(testdataset, batch_size=32, shuffle=False, num_workers=0,drop_last=True)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "print('Accuracy: {}'.format(test(model, test_loader, criterion)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_min_row(csr_mat):\n",
    "    ret = np.zeros(csr_mat.shape[0])\n",
    "    ret[np.diff(csr_mat.indptr) != 0] = np.minimum.reduceat(csr_mat.data,csr_mat.indptr[:-1][np.diff(csr_mat.indptr)>0])\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute rips persistent homology (up to 1st dimension) over entire network \n",
    "# using (sparse) adjacency matrix as distance matrix.\n",
    "\n",
    "def sparse_min_row(csr_mat):\n",
    "    ret = np.zeros(csr_mat.shape[0])\n",
    "    ret[np.diff(csr_mat.indptr) != 0] = np.minimum.reduceat(csr_mat.data,csr_mat.indptr[:-1][np.diff(csr_mat.indptr)>0])\n",
    "    return ret\n",
    "\n",
    "sps = nx.to_scipy_sparse_matrix(NNG.G)\n",
    "mrs = sparse_min_row(sps)\n",
    "sps.setdiag(mrs)\n",
    "\n",
    "\n",
    "%time rips = ripser(sps, distance_matrix=True, maxdim=1)\n",
    "rips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot persistence diagram in dimensions 0 and 1 (on same axes).\n",
    "# points at infinity (homology groups) are plotted on the dotted \n",
    "# line which represents the point \\infty.\n",
    "persim.plot_diagrams(rips['dgms'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruned LT Homology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the LT model.\n",
    "model_lt = torch.load(model_loc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test this model's accuracy.\n",
    "print('Accuracy: {}'.format(test(model_lt, test_loader, criterion)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NNGLT = nn_graph.NNGraph()\n",
    "NNGLT.parameter_graph(model_lt, param_info, input_size, ignore_zeros=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute rips persistent homology (up to 1st dimension) over entire network \n",
    "# using (sparse) adjacency matrix as distance matrix.\n",
    "sps = nx.to_scipy_sparse_matrix(NNGLT.G)\n",
    "mrs = sparse_min_row(sps)\n",
    "sps.setdiag(mrs)\n",
    "%time rips_lt = ripser(sps, distance_matrix=True, maxdim=1)\n",
    "rips_lt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot persistence diagram in dimensions 0 and 1 (on same axes).\n",
    "# points at infinity (homology groups) are plotted on the dotted \n",
    "# line which represents the point \\infty.\n",
    "persim.plot_diagrams(rips_lt['dgms'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the bottleneck distance between networks and plot the implicit matching. \n",
    "# bottleneck distance is defined as the distance between the farthest-apart matched points. \n",
    "# NOTE: the persim package ignores points at infinity, so this calculation still returns \n",
    "# a bounded result. Technically, the bottleneck distance between the two networks is \\infty. \n",
    "# %time distance_bottleneck = persim.bottleneck(rips['dgms'][0], rips_lt['dgms'][0], matching=False)\n",
    "# persim.bottleneck_matching(rips['dgms'][0], rips_lt['dgms'][0], matching, D, labels=['FC $H_0$', 'LT $H_0$'])\n",
    "# print('Bottleneck Distance: {}'.format(distance_bottleneck))\n",
    "# %time sliced = persim.sliced_wasserstein(rips['dgms'][0], rips_lt['dgms'][0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
